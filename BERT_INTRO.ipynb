{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e207d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import pipeline, BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# https://github.com/jessevig/bertviz\n",
    "from bertviz import head_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92dc17da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of BERT base vocabulary: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f'Length of BERT base vocabulary: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cd67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 3722, 6251, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"A simple sentence!\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173262dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a simple sentence! [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode will re-construct the sentence with the added [CLS] and [SEP] token\n",
    "tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67162eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb78094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My friend told me about this class and I love it so far! She was right.. Num tokens: 20\n",
      "Token: 101, subword: [CLS]\n",
      "Token: 2026, subword: my\n",
      "Token: 2767, subword: friend\n",
      "Token: 2409, subword: told\n",
      "Token: 2033, subword: me\n",
      "Token: 2055, subword: about\n",
      "Token: 2023, subword: this\n",
      "Token: 2465, subword: class\n",
      "Token: 1998, subword: and\n",
      "Token: 1045, subword: i\n",
      "Token: 2293, subword: love\n",
      "Token: 2009, subword: it\n",
      "Token: 2061, subword: so\n",
      "Token: 2521, subword: far\n",
      "Token: 999, subword: !\n",
      "Token: 2016, subword: she\n",
      "Token: 2001, subword: was\n",
      "Token: 2157, subword: right\n",
      "Token: 1012, subword: .\n",
      "Token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# A nicer printout  of token ids and token strings\n",
    "\n",
    "print(f'Text: {text}. Num tokens: {len(tokens)}')\n",
    "for t in tokens:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c485ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a vanilla BERT-base model. \n",
    "# Note we have to specify uncased because the vocab size / pre-trained vectors are different\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd67893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nword embedding : context free word embedding\\nposition embedding : embedding position\\ntoken type embedding : 0 or 1 used to lookup the segment embedding\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "word embedding : context free word embedding\n",
    "position embedding : embedding position\n",
    "token type embedding : 0 or 1 used to lookup the segment embedding\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4649ee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 199 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "embeddings.word_embeddings.weight                       (30522, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "named_params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(named_params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in named_params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in named_params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in named_params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c47a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
